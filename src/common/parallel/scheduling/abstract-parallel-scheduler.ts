import {IParallelJob, IDefaultInitializedParallelOptions, IParallelOperation} from "../";
import {ITask} from "../../task/task";
import {ITaskDefinition} from "../../task/task-definition";
import {IParallelTaskDefinition} from "../parallel-task-definition";
import {FunctionCallSerializer} from "../../function/function-call-serializer";
import {FunctionCall} from "../../function/function-call";
import {IParallelJobScheduler} from "./parallel-job-scheduler";
import {ParallelWorkerFunctionIds} from "../slave/parallel-worker-functions";
import {IParallelJobDefinition} from "../slave/parallel-job-executor";
import {flattenArray} from "../../util/arrays";
import {isSerializedFunctionCall} from "../../function/serialized-function-call";

export abstract class AbstractParallelScheduler implements IParallelJobScheduler {
    public schedule<TResult>(job: IParallelJob): ITask<TResult>[] {
        const taskDefinitions = this.getTaskDefinitions(job);
        return taskDefinitions.map(taskDefinition => job.options.threadPool.run(taskDefinition));
    }

    /**
     * Returns the suggested scheduling for the given number of values - while concerning the passed in options.
     * @param totalNumberOfValues the total number of values to be processed by the parallel operation chain
     * @param options the parallel options provided for this operation chain
     */
    public abstract getScheduling(totalNumberOfValues: number, options: IDefaultInitializedParallelOptions): IParallelTaskScheduling;

    private getTaskDefinitions(job: IParallelJob): ITaskDefinition[] {
        const scheduling = this.getScheduling(job.generator.length, job.options);
        const functionCallSerializer = job.options.functionCallSerializer;

        const environments = job.environment.toJSON(functionCallSerializer);
        const operations = this.serializeOperations(job.operations, functionCallSerializer);
        const commonFunctionIds = [ParallelWorkerFunctionIds.PARALLEL_JOB_EXECUTOR].concat(flattenArray(operations.map(operation => [operation.iteratee.functionId, operation.iterator.functionId])));

        environments.forEach(provider => {
            if (isSerializedFunctionCall(provider)) {
                commonFunctionIds.push(provider.functionId);
            }
        });

        const taskDefinitions: ITaskDefinition[] = [];
        for (let i = 0; i < scheduling.numberOfTasks; ++i) {
            const generator = job.generator.serializeSlice(i, scheduling.valuesPerTask, functionCallSerializer);

            const processParams: IParallelJobDefinition = {
                environments,
                generator,
                operations,
                taskIndex: i,
                valuesPerTask: scheduling.valuesPerTask
            };

            const taskDefinition: IParallelTaskDefinition = {
                main: functionCallSerializer.serializeFunctionCall(FunctionCall.createUnchecked(ParallelWorkerFunctionIds.PARALLEL_JOB_EXECUTOR, processParams)),
                taskIndex: i,
                usedFunctionIds: [generator.functionId].concat(commonFunctionIds),
                valuesPerTask: scheduling.valuesPerTask
            };

            taskDefinitions.push(taskDefinition);
        }
        return taskDefinitions;
    }

    private serializeOperations(operations: IParallelOperation[], functionCallSerializer: FunctionCallSerializer) {
        return operations.map(operation => ({
            iteratee: functionCallSerializer.serializeFunctionCall(operation.iteratee),
            iterator: functionCallSerializer.serializeFunctionCall(operation.iterator)
        }));
    }
}

/**
 * Defines how a parallel task should be scheduled on the thread pool
 */
export interface IParallelTaskScheduling {

    /**
     * How many number of tasks should be created to perform the operation
     */
    numberOfTasks: number;

    /**
     * How many values to process by each task (at most)
     */
    valuesPerTask: number;
}
